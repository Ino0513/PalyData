{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d23187-2b5d-4ef4-aed4-1792210d2c54",
   "metadata": {},
   "source": [
    "# [Huggingface ](https://huggingface.co)\n",
    "\n",
    "Hugging Face는 인공지능(AI)과 자연어 처리(NLP) 분야에서 혁신적인 도구와 모델을 제공하는 AI 스타트업이다. 2016년에 설립된 이 회사는 주로 오픈소스 라이브러리와 사전 학습된 NLP 모델을 제공을 제공한다.\n",
    "\n",
    "## 주요 서비스\n",
    "1. **Transformers 라이브러리**: 다양한 NLP 작업을 위한 사전 학습된 모델(Pretrained Model)들을 제공하는 라이브러리로, BERT, GPT-3, T5 등의 모델과 huggingface 생태계의 다양한 서비스들을 이용할 수있는 파이썬 기반의 오픈소스 라이브러리.\n",
    "3. **Huggingface Model Hub**: 사용자들이 자신이 훈련한 모델들을 공유 할 수 있는 git 기반 플랫폼. \n",
    "2. **Datasets 라이브러리**: 다양한 NLP 데이터셋을 손쉽게 활용할 수 있도록 하는 라이브러리로, 데이터 전처리 및 관리에 유용하다.\n",
    "4. **API 서비스**: 모델 배포를 위한 API 서비스를 제공하여, 사용자가 쉽게 모델을 배포하고 활용할 수 있도록 지원한다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2fd763-772e-4ed8-99f9-ce8115756cb7",
   "metadata": {},
   "source": [
    "## Transformers 라이브러리\n",
    "\n",
    "Hugging Face의 Transformers는 자연어 처리(NLP) 분야에서 가장 널리 사용되는 라이브러리 중 하나로, 다양한 사전 학습된 모델을 제공하여 연구자들과 개발자들이 손쉽게 NLP 애플리케이션을 구축할 수 있도록 돕는다. 이 라이브러리는 언어 모델링, 텍스트 생성, 번역, 감성 분석, 질문 응답 시스템과 같은 다양한 NLP 작업을 수행할 수 있도록 설계되었다.\n",
    "\n",
    "### 주요 특징\n",
    "\n",
    "1. **광범위한 모델 지원**: BERT, GPT-3, RoBERTa, T5 등 다양한 최첨단 모델들을 지원한다. 이러한 모델들은 텍스트 분류, 감성 분석, 질문 답변, 텍스트 생성 등 다양한 NLP 작업에 활용될 수 있다.\n",
    "2. **파이토치(PyTorch)와 텐서플로우(TensorFlow) 호환**: Transformers 라이브러리는 두 가지 주요 딥러닝 프레임워크와 호환되어, 사용자가 자신의 선호에 따라 선택하여 사용할 수 있다. \n",
    "3. **사용자 친화적인 인터페이스**: 간단하고 직관적인 API를 통해 복잡한 모델을 쉽게 불러오고 사용할 수 있다. 이를 통해 빠르게 프로토타입을 만들고 실험할 수 있다.\n",
    "4. **모델 허브**: HuggingFace Model Hub를 통해 공유되는 수많은 모델들을 Transformers 라이브러리를 이용해 다운받아서 추론까지 쉽게 사용할 수있다.\n",
    "5. **자동화된 파이프라인**: 파이프라인(pipeline) API를 사용하면 몇 줄의 코드로 다양한 NLP 작업을 수행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef06541a-3111-46cb-a3be-4b50d8267e1a",
   "metadata": {},
   "source": [
    "# Transformers 를 이용해 Backbone 사용\n",
    "\n",
    "## Transformers 설치\n",
    "- `pip install transformers`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3817d6ef-7239-4738-89c1-1d6a64d8a8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
      "     ---------------------------------------- 0.0/43.8 kB ? eta -:--:--\n",
      "     ---------------------------- ----------- 30.7/43.8 kB 1.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 43.8/43.8 kB 714.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\anaconda3\\envs\\env01\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Collecting huggingface-hub<1.0,>=0.23.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\anaconda3\\envs\\env01\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\env01\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\anaconda3\\envs\\env01\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\anaconda3\\envs\\env01\\lib\\site-packages (from transformers) (2024.5.15)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\anaconda3\\envs\\env01\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.3-cp312-none-win_amd64.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\anaconda3\\envs\\env01\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\anaconda3\\envs\\env01\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\envs\\env01\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.11.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\env01\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\env01\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\anaconda3\\envs\\env01\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\env01\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\anaconda3\\envs\\env01\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Downloading transformers-4.41.2-py3-none-any.whl (9.1 MB)\n",
      "   ---------------------------------------- 0.0/9.1 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 0.8/9.1 MB 26.1 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 4.4/9.1 MB 56.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.6/9.1 MB 68.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.1/9.1 MB 64.4 MB/s eta 0:00:00\n",
      "Downloading huggingface_hub-0.23.3-py3-none-any.whl (401 kB)\n",
      "   ---------------------------------------- 0.0/401.7 kB ? eta -:--:--\n",
      "   --------------------------------------- 401.7/401.7 kB 26.1 MB/s eta 0:00:00\n",
      "Downloading safetensors-0.4.3-cp312-none-win_amd64.whl (289 kB)\n",
      "   ---------------------------------------- 0.0/289.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 289.4/289.4 kB ? eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp312-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   ------------------------- -------------- 1.4/2.2 MB 30.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 27.9 MB/s eta 0:00:00\n",
      "Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.23.3 safetensors-0.4.3 tokenizers-0.19.1 transformers-4.41.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d4825a-fe9f-4d1e-b9e0-7bb78745b840",
   "metadata": {},
   "source": [
    "### Tokenizer, Model Loading\n",
    "- Huggingface 모델 허브에서 제공하는 처리 모델을 다운받아 로딩한다.\n",
    "- 다운로드된 모델은 `사용자 home 디렉토리\\.cache\\huggingface` 에 저장된다.\n",
    "- 미리 학습된 언어 모델을 다운받아 사용할 때는 그 언어모델이 사용한 tokenizer를 같이 받아서 사용한다.\n",
    "\n",
    "### [Auto Classes](https://huggingface.co/docs/transformers/model_doc/auto)\n",
    "- Huggingface 에서 제공하는 다양한 모델들은 손쉽게 불러오고 사용할 수 있도록 설계된 유틸리티 클래스들을 말한다.\n",
    "- 미리 학습된 특정 모델의 이름(모델 허브상에서 제공되는 이름)이나 저장된 local 경로를 제공하면 해당 모델에 맞는 적절한 클래스와 구성 요소를 자동으로 로드한다.\n",
    "- 사용자는 모델을 사용하기 위한 정확한 클래스를 몰라도 쉽게 다양한 종류의 모델을 사용할 수있다.\n",
    "\n",
    "#### 주요 Auto Class\n",
    "\n",
    "1. **AutoModel**\n",
    "   - 주어진 모델 이름에 맞는 사전 학습된 모델을 자동으로 로드한다.\n",
    "   - 예: `AutoModel.from_pretrained(\"bert-base-uncased\")`: BERT 모델을 로드한다.\n",
    "\n",
    "2. **AutoTokenizer**\n",
    "   - 해당 모델에 적합한 토크나이저를 자동으로 로드한다.\n",
    "   - 예: `AutoTokenizer.from_pretrained(\"bert-base-uncased\")`: BERT 모델에 맞는 토크나이저를 로드한다.\n",
    "\n",
    "3. **AutoConfig**\n",
    "   - 모델의 설정(config)을 자동으로 로드한다. 모델 설정에는 모델의 하이퍼파라미터와 모델 구조 정보가 포함된다. 이 설정을 이용해 모델을 생성할 수있다.\n",
    "   - 예: `AutoConfig.from_pretrained(\"bert-base-uncased\")`\n",
    "\n",
    "4. **AutoModelForSequenceClassification**\n",
    "   - 시퀀스(Text) 분류 작업을 위한 모델을 자동으로 로드한다.\n",
    "   - 예: `AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")`\n",
    "\n",
    "5. **AutoModelForQuestionAnswering**\n",
    "   - 질문-응답 작업을 위한 모델을 자동으로 로드한다.\n",
    "   - 예: `AutoModelForQuestionAnswering.from_pretrained(\"bert-base-uncased\")`\n",
    "\n",
    "6. **AutoModelForTokenClassification**\n",
    "   - 토큰 분류 작업(예: 개체명 인식)을 위한 모델을 자동으로 로드한다.\n",
    "   - 예: `AutoModelForTokenClassification.from_pretrained(\"bert-base-uncased\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ffab593-17fa-4676-922d-336397744e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20cde7bd9f5d4b3d9efb5e7b3331a798",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\env01\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\USER\\anaconda3\\envs\\env01\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e86c0086ccab47fe96f3b84e64d8a8aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c3996bfce043fcb5d464606d53a7e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca02a489a8a043c79e3d4924d6ef41f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.tokenization_bert_fast.BertTokenizerFast'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7a210825d6401990fd13f23e4ac238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.modeling_bert.BertModel'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\env01\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.bert.configuration_bert.BertConfig'>\n",
      "<class 'transformers.models.bert.modeling_bert.BertModel'>\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel, AutoConfig, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(tokenizer))\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(model))\n",
    "conf = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "print(type(conf))\n",
    "model2 = AutoModel.from_config(conf)\n",
    "print(type(model2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fe70881-f8e1-4eff-b49e-ccc0a8200dd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e9bcc7c487d45b1b0a13cde8a91ad5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\env01\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--openai-community--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edad26cf63714c0c82a2444c8978b3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.models.gpt2.modeling_gpt2.GPT2Model'>\n"
     ]
    }
   ],
   "source": [
    "gpt_model = AutoModel.from_pretrained('openai-community/gpt2')\n",
    "print(type(gpt_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5a02d44-7271-4db4-b8c0-7c14037dce3a",
   "metadata": {},
   "source": [
    "## kcbert\n",
    "- BERT 한글 텍스트 학습 모델\n",
    "- https://huggingface.co/beomi/kcbert-base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0af872e-410e-4a3c-8ec3-2b1e1a680ad6",
   "metadata": {},
   "source": [
    "### 토크나이저 모델 load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fc7e8c2-dc3d-4f39-84a7-39e6bbc38fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60f091a63821464f92f2d9f5dd73fa29",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\USER\\anaconda3\\envs\\env01\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--beomi--kcbert-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3d68b7300e430186196a4e1ea2f598",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/619 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd97ec5461034d1c8a269079f83b54cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/250k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2fadb501454991886f4098994f37c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"beomi/kcbert-base\")  # BertTokenizer: WordPiece 방식 토크나이저\n",
    "model = AutoModel.from_pretrained(\"beomi/kcbert-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5087a1-04bb-4b36-912f-c1c5bdd77101",
   "metadata": {},
   "source": [
    "### 입력값 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b46b85a-8123-4108-a087-435d8c1d9218",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Ture is not a valid PaddingStrategy, please select one of ['longest', 'max_length', 'do_not_pad']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#  batch_encode_plus 와 동일 \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m### call함수 호출\u001b[39;00m\n\u001b[0;32m      4\u001b[0m sentences \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m안녕하세요. 반갑습니다.\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKcELECTRA가 릴리즈 되었습니다!\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKcELECTRA는 보다 더 많은 데이터셋, 그리고 더 큰 General vocab을 통해 KcBERT 대비 모든 태스크에서 더 높은 성능을 보입니다.\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m----> 6\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mTure\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m#`True` or `'longest'`, `'max_length'`, `False` or `'do_not_pad'`(default)\u001b[39;49;00m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# np: 넘파이, tf: TF, pt: pytorch\u001b[39;49;00m\n\u001b[0;32m     11\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m features\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\env01\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2883\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[1;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2881\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[0;32m   2882\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[1;32m-> 2883\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2885\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\env01\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2969\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[1;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2964\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2965\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch length of `text`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not match batch length of `text_pair`:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2966\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(text_pair)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2967\u001b[0m         )\n\u001b[0;32m   2968\u001b[0m     batch_text_or_text_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(text, text_pair)) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m text\n\u001b[1;32m-> 2969\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2970\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_text_or_text_pairs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2971\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2978\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2980\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2981\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2982\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2983\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2986\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2987\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2988\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_plus(\n\u001b[0;32m   2990\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m   2991\u001b[0m         text_pair\u001b[38;5;241m=\u001b[39mtext_pair,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3007\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3008\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\env01\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:3151\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.batch_encode_plus\u001b[1;34m(self, batch_text_or_text_pairs, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   3134\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3135\u001b[0m \u001b[38;5;124;03mTokenize and prepare for the model a list of sequences or a list of pairs of sequences.\u001b[39;00m\n\u001b[0;32m   3136\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3147\u001b[0m \u001b[38;5;124;03m        details in `encode_plus`).\u001b[39;00m\n\u001b[0;32m   3148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3150\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m-> 3151\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_padding_truncation_strategies\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3157\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3158\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_batch_encode_plus(\n\u001b[0;32m   3161\u001b[0m     batch_text_or_text_pairs\u001b[38;5;241m=\u001b[39mbatch_text_or_text_pairs,\n\u001b[0;32m   3162\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3177\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3178\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\env01\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2725\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._get_padding_truncation_strategies\u001b[1;34m(self, padding, truncation, max_length, pad_to_multiple_of, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   2723\u001b[0m     padding_strategy \u001b[38;5;241m=\u001b[39m PaddingStrategy\u001b[38;5;241m.\u001b[39mLONGEST  \u001b[38;5;66;03m# Default to pad to the longest sequence in the batch\u001b[39;00m\n\u001b[0;32m   2724\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, PaddingStrategy):\n\u001b[1;32m-> 2725\u001b[0m     padding_strategy \u001b[38;5;241m=\u001b[39m \u001b[43mPaddingStrategy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2726\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(padding, PaddingStrategy):\n\u001b[0;32m   2727\u001b[0m     padding_strategy \u001b[38;5;241m=\u001b[39m padding\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\env01\\Lib\\enum.py:757\u001b[0m, in \u001b[0;36mEnumType.__call__\u001b[1;34m(cls, value, names, module, qualname, type, start, boundary, *values)\u001b[0m\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m names \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _not_given:\n\u001b[0;32m    756\u001b[0m         value \u001b[38;5;241m=\u001b[39m (value, names) \u001b[38;5;241m+\u001b[39m values\n\u001b[1;32m--> 757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__new__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;66;03m# otherwise, functional API: we're creating a new Enum type\u001b[39;00m\n\u001b[0;32m    759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m names \u001b[38;5;129;01mis\u001b[39;00m _not_given \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    760\u001b[0m     \u001b[38;5;66;03m# no body? no data-type? possibly wrong usage\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\env01\\Lib\\enum.py:1179\u001b[0m, in \u001b[0;36mEnum.__new__\u001b[1;34m(cls, value)\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(exc, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[0;32m   1178\u001b[0m             exc\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;241m=\u001b[39m ve_exc\n\u001b[1;32m-> 1179\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m   1180\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1181\u001b[0m     \u001b[38;5;66;03m# ensure all variables that could hold an exception are destroyed\u001b[39;00m\n\u001b[0;32m   1182\u001b[0m     exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\env01\\Lib\\enum.py:1156\u001b[0m, in \u001b[0;36mEnum.__new__\u001b[1;34m(cls, value)\u001b[0m\n\u001b[0;32m   1154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1155\u001b[0m     exc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1156\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_missing_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1158\u001b[0m     exc \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\envs\\env01\\Lib\\site-packages\\transformers\\utils\\generic.py:495\u001b[0m, in \u001b[0;36mExplicitEnum._missing_\u001b[1;34m(cls, value)\u001b[0m\n\u001b[0;32m    493\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_missing_\u001b[39m(\u001b[38;5;28mcls\u001b[39m, value):\n\u001b[1;32m--> 495\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    496\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, please select one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_value2member_map_\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    497\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Ture is not a valid PaddingStrategy, please select one of ['longest', 'max_length', 'do_not_pad']"
     ]
    }
   ],
   "source": [
    "#  batch_encode_plus 와 동일 \n",
    "### call함수 호출\n",
    "\n",
    "sentences = ['안녕하세요. 반갑습니다.','KcELECTRA가 릴리즈 되었습니다!','KcELECTRA는 보다 더 많은 데이터셋, 그리고 더 큰 General vocab을 통해 KcBERT 대비 모든 태스크에서 더 높은 성능을 보입니다.']\n",
    "\n",
    "features = tokenizer(\n",
    "    sentences,\n",
    "    max_length=10,\n",
    "    padding= 'max_length',  #`True` or `'longest'`, `'max_length'`, `False` or `'do_not_pad'`(default)\n",
    "    return_tensors=\"pt\", # np: 넘파이, tf: TF, pt: pytorch\n",
    ")\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "bc7c8a6c-3a62-4bde-9e0b-b6f661cee605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    2, 19017,  8482,     3,     0,     0,     0,     0,     0,     0],\n",
      "        [    2, 15830,     5,     3,     0,     0,     0,     0,     0,     0],\n",
      "        [    2,  9909,  2483,  2688,  4755,  8186,    32,     3,     0,     0]])\n",
      "--------------------\n",
      "tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0]])\n",
      "--------------------\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "# input_ids: 워드피스 토큰화 -> token index로 변환한 결과\n",
    "pprint(features['input_ids']) # 2: 문장시작(CLS), 3: 문장 끝(SEP), 0: 패딩(PAD)  표현하는 special token,  (max_length=10으로 했으므로 token idx가 10개로 구성됨.)\n",
    "print(\"-\"*20)\n",
    "# attention mask: 일반 토큰인지 패딩인지 mask 행렬. padding 아닌 token => 1, padding인 token => 0 \n",
    "pprint(features['attention_mask'])\n",
    "print(\"-\"*20)\n",
    "# token_type_ids: segment 정보 (token pair일때 첫번째는 0, 두번째는 1로 채워진다. -> 이 둘을 하나의 tokens로 묶어서 반환하는데 그 구분-https://hyen4110.tistory.com/89 확인) 한개 문장일 경우 모두 0이된다.\n",
    "pprint(features['token_type_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f357f6-73ce-4a1d-9fce-84424b642d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode(\"안녕하세요. 저는 홍길동입니다.\")  # -> [2, 19017, 8482, 3]\n",
    "tokenizer.decode([2, 19017, 8482, 3]) # -> [CLS] 안녕하세요 [SEP]\n",
    "# result = tokenizer.batch_encode_plus(sentences)# 한번에 여러개\n",
    "print(type(result))\n",
    "result['input_ids']  # list - 문장 -> 토큰 id 리스트\n",
    "result['token_type_ids'] # pair 일때 0번, 1번 어떤 문장의 토큰인지\n",
    "result['attention_mask']  # 토큰인지 패딩인지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09aaa025-17c9-46ad-b708-f2729cb2a95e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'transformers.tokenization_utils_base.BatchEncoding'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.get_vocab()\n",
    "# tokenizer.vocab_size\n",
    "# tokenizer.convert_ids_to_tokens(2)\n",
    "# tokenizer.convert_tokens_to_ids(\"내일\")\n",
    "# tokenizer.convert_tokens_to_string([\"내일\", \"또\", \"봐\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f4f1c111-09d8-42f4-b2e6-1467b02dc671",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('[SEP]', '[CLS]')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.ids_to_tokens[3], tokenizer.ids_to_tokens[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19acd0fe-5952-4ee6-97ee-e4fcf41ec093",
   "metadata": {},
   "source": [
    "### BERT 모델을 이용해 문장수준 Vector생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9f0eaf52-43ed-458e-97b8-4835b183ef70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['input_ids', 'token_type_ids', 'attention_mask'])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "240b332b-0827-4388-a394-ede6cd1ee213",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(**features) # 'input_ids', 'token_type_ids', 'attention_mask' 입력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "054bb72f-f5bd-49cf-a98a-e32b3222f04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4393e7b6-0b57-498e-a4c2-27782aa1f6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'last_hidden_state' : rnn의 outputs 같이 모든 토큰에 대한 출력 feature\n",
    "# 'pooler_output' : rnn의 hidden_state 같이 마지막 token에 대한 처리값. 입력 문장, 텍스트에 대한 단일 vector 표현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b0f2a839-9fa3-4c22-b3a5-21bc2f7e6684",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 10, 768])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.last_hidden_state.shape)  # feature값\n",
    "# [3:입력문장수, 10:토큰수-max_length설정한 것, 768: vector] -> 10개 토큰이 768개 features로 변환된 것. \n",
    "##### 이것은 token별 vector가 나오므로 **개체명 인식 task** 와 같이 단어별로 수행해야 하는 task에 활용할 수있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a206af7-a0c1-4025-804d-8012b9eae71c",
   "metadata": {},
   "source": [
    "### Model 추론결과\n",
    "- **last_hidden_state**\n",
    "    - 모든 token들에 대한 feature\n",
    "    - 출력이 **many**인 작업에 사용한다.\n",
    "- **pooler_output**\n",
    "    - 입력 문장, 텍스트에 대한 단일 vector 표현을 말한다.\n",
    "    - BERT는 \\[CLS\\] 토큰(문장시작)의 출력 임베딩을 사용해서 생성한다.\n",
    "        - \\[CLS\\] 토큰은 입력 시퀀스의 맨 앞에 추가되며, 문장의 전체적인 의미를 얻기 위해 설계된 특수 토큰임.\n",
    "    - pooler의 동작과정\n",
    "        1. \\[CLS\\] 토큰의 Embedding 추출: Text sequence가 BERT 모델을 통과하면 각 토큰에 대한 임베딩이 생성된다.(last_hidden_state). 이 중 첫번째 토큰인 \\[CLS\\] 토큰의 임베딩을 선택한다.\n",
    "        2. Embedding Pooling Layer로 1의 임베딩 전달: Pooling Layer는 Pooler를 생성하는 Fully Connected Layer(선형변환)와 Activation 함수(주로 Tanh를 사용한다.)를 통과시킨다.\n",
    "        3. pooler output: 2의 과정을 통과한 결과가 최종 **pooler_output**이 된다. 이 값이 **전체 입력 시퀀스에 대한 요약정보로 사용된다.**\n",
    "    - **pooler output 이용**   \n",
    "       -  이 값은 문장을 입력받아 처리하는 task(ex: 문서분류-감정분석,문장카테고리분류, 문장유사도 분석)의 입력으로 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "756d830c-6bd5-47d1-b961-03724a12d6e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 768])\n"
     ]
    }
   ],
   "source": [
    "print(outputs.pooler_output.shape)\n",
    "### CLS(문장시작-2) 토큰이 변환된 Vector"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
